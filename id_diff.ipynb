{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6ea427",
   "metadata": {},
   "source": [
    "# ID Comparison Notebook\n",
    "\n",
    "This notebook will be able to load two CSV/TXT files, compare ID values, shows matches and highlights non-matching IDs, and saves reports.\n",
    "\n",
    "## How to use\n",
    "- Set `file1_path` and `file2_path` to your files.\n",
    "- If files have headers, set `id_column` to the ID column name and keep `use_header = True`.\n",
    "- If files do NOT have headers, set `use_header = False` and set `id_index` to the 0-based index of the ID column.\n",
    "- Delimiters are auto-detected; you can override with `delimiter1`/`delimiter2`.\n",
    "- Run the cells in order. Reports will be saved next to your input files as `id_matches.csv`, `id_only_in_file1.csv`, `id_only_in_file2.csv`, and `id_diff_summary.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd5eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below will install the required packages if they are not already installed\n",
    "%pip install --upgrade pip\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install jinja2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6c84f",
   "metadata": {},
   "source": [
    "## File loading\n",
    "To update the files for the specific items that are needing to be checked for the differences.\n",
    "Add the folder path for where the files are being stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87461f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration: set your file paths and ID column\n",
    "file1_path = r'./file1.csv'  # e.g., 'd:\\Scripts\\listA.csv'\n",
    "file2_path = r'./file2.csv'  # e.g., 'd:\\Scripts\\listB.txt'\n",
    "\n",
    "# If your files have a header row with the ID column name, set id_column\n",
    "id_column = 'ID'  # e.g., 'id', 'accountNumber'\n",
    "\n",
    "# If files do NOT have headers, set this to the 0-based index of the ID column and set header=None\n",
    "use_header = True  # Set to False if there is no header\n",
    "id_index = 0  # only used when use_header=False\n",
    "\n",
    "# Optional: delimiter hints; set to None for auto-detection\n",
    "delimiter1 = None\n",
    "delimiter2 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aa98ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "def validate_delimiter(delimiter: Optional[str]) -> str:\n",
    "    \"\"\"Validate and sanitize delimiter value\"\"\"\n",
    "    if delimiter is None:\n",
    "        return None  # Signal auto-detection\n",
    "    \n",
    "    # Handle common string representations\n",
    "    if delimiter == '\\\\t' or delimiter == 'tab':\n",
    "        return '\\t'\n",
    "    \n",
    "    # Reject invalid delimiters (line endings)\n",
    "    if delimiter in ['\\n', '\\r', '\\\\n', '\\\\r']:\n",
    "        return None  # Force re-detection\n",
    "    \n",
    "    # Ensure it's a string\n",
    "    delimiter = str(delimiter)\n",
    "    \n",
    "    # Check for empty or invalid\n",
    "    if len(delimiter) == 0:\n",
    "        return None  # Signal auto-detection\n",
    "    \n",
    "    return delimiter\n",
    "\n",
    "def detect_delimiter(file_path: str, default: str = ',') -> str:\n",
    "    \"\"\"Improved delimiter detection with fallback logic\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', newline='', encoding='utf-8') as f:\n",
    "            sample = f.read(4096)\n",
    "            if not sample.strip():\n",
    "                return default\n",
    "            \n",
    "            # Try csv.Sniffer with restricted delimiters\n",
    "            try:\n",
    "                dialect = csv.Sniffer().sniff(sample, delimiters=',\\t|;')\n",
    "                detected = dialect.delimiter\n",
    "                # Validate it's not a line ending\n",
    "                if detected not in ['\\n', '\\r']:\n",
    "                    return detected\n",
    "            except csv.Error:\n",
    "                pass\n",
    "            \n",
    "            # Fallback: count common delimiters\n",
    "            first_line = sample.split('\\n')[0] if '\\n' in sample else sample[:200]\n",
    "            counts = {\n",
    "                ',': first_line.count(','),\n",
    "                '\\t': first_line.count('\\t'),\n",
    "                '|': first_line.count('|'),\n",
    "                ';': first_line.count(';')\n",
    "            }\n",
    "            \n",
    "            # Return delimiter with highest count (if > 0)\n",
    "            max_delim = max(counts, key=counts.get)\n",
    "            if counts[max_delim] > 0:\n",
    "                return max_delim\n",
    "            \n",
    "            return default\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: delimiter detection failed: {e}\")\n",
    "        return default\n",
    "\n",
    "def load_file(file_path: str, delimiter_hint: Optional[str], use_header: bool, id_column: Optional[str], id_index: Optional[int]) -> pd.DataFrame:\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f'File not found: {file_path}')\n",
    "    \n",
    "    # Validate delimiter first\n",
    "    delimiter = validate_delimiter(delimiter_hint)\n",
    "    \n",
    "    # Auto-detect if needed\n",
    "    if delimiter is None:\n",
    "        delimiter = detect_delimiter(file_path)\n",
    "    \n",
    "    # Final safety check\n",
    "    if not delimiter or len(delimiter) == 0 or delimiter in ['\\n', '\\r']:\n",
    "        delimiter = ','\n",
    "        print(f\"Warning: Defaulting to comma delimiter for {os.path.basename(file_path)}\")\n",
    "    \n",
    "    header = 0 if use_header else None\n",
    "    df = pd.read_csv(file_path, sep=delimiter, header=header, dtype=str, engine='python')\n",
    "    \n",
    "    # Normalise whitespace and strip IDs\n",
    "    if use_header:\n",
    "        if id_column is None or id_column not in df.columns:\n",
    "            raise ValueError(f'ID column \"{id_column}\" not found. Available columns: {list(df.columns)}')\n",
    "        df[id_column] = df[id_column].astype(str).str.strip()\n",
    "        df_ids = df[[id_column]].copy()\n",
    "        df_ids.columns = ['ID']\n",
    "    else:\n",
    "        if id_index is None or id_index < 0 or id_index >= df.shape[1]:\n",
    "            raise ValueError(f'Invalid id_index {id_index}; file has {df.shape[1]} columns')\n",
    "        df_ids = df.iloc[:, [id_index]].copy()\n",
    "        df_ids.columns = ['ID']\n",
    "        df_ids['ID'] = df_ids['ID'].astype(str).str.strip()\n",
    "    \n",
    "    # Drop blank IDs\n",
    "    df_ids = df_ids[df_ids['ID'].astype(str).str.len() > 0]\n",
    "    \n",
    "    # Remove duplicates within each file\n",
    "    df_ids = df_ids.drop_duplicates().reset_index(drop=True)\n",
    "    return df_ids\n",
    "\n",
    "print('Libraries loaded. Ready to process files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7068e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuzzy normalisation configuration\n",
    "enable_normalization = True  # Set to False to disable\n",
    "\n",
    "def normalize_id(x: str) -> str:\n",
    "    # Remove common separators and whitespace, uppercase for case-insensitive matching\n",
    "    if pd.isna(x):\n",
    "        return ''\n",
    "    return (str(x)\n",
    "            .replace('-', '')\n",
    "            .replace('_', '')\n",
    "            .replace(' ', '')\n",
    "            .strip()\n",
    "            .upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b51fcf",
   "metadata": {},
   "source": [
    "## Pre-validation Testing\n",
    "Let's test for handling delimiter issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7fb053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-validation Test of delimiter before passing to pandas (IMPROVED)\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "def validate_delimiter(delimiter: Optional[str]) -> str:\n",
    "    \"\"\"Validate and sanitize delimiter value\"\"\"\n",
    "    if delimiter is None:\n",
    "        return None  # Signal auto-detection\n",
    "    \n",
    "    # Handle common string representations\n",
    "    if delimiter == '\\\\t' or delimiter == 'tab':\n",
    "        return '\\t'\n",
    "    \n",
    "    # Reject invalid delimiters (line endings)\n",
    "    if delimiter in ['\\n', '\\r', '\\\\n', '\\\\r']:\n",
    "        return None  # Force re-detection\n",
    "    \n",
    "    # Ensure it's a string\n",
    "    delimiter = str(delimiter)\n",
    "    \n",
    "    # Check for empty or invalid\n",
    "    if len(delimiter) == 0:\n",
    "        return None  # Signal auto-detection\n",
    "    \n",
    "    return delimiter\n",
    "\n",
    "def detect_delimiter_v2(file_path: str) -> str:\n",
    "    \"\"\"Improved delimiter detection with fallback logic\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', newline='', encoding='utf-8') as f:\n",
    "            sample = f.read(4096)\n",
    "            if not sample.strip():\n",
    "                return ','\n",
    "            \n",
    "            # Try csv.Sniffer with restricted delimiters\n",
    "            try:\n",
    "                dialect = csv.Sniffer().sniff(sample, delimiters=',\\t|;')\n",
    "                detected = dialect.delimiter\n",
    "                # Validate it's not a line ending\n",
    "                if detected not in ['\\n', '\\r']:\n",
    "                    return detected\n",
    "            except csv.Error:\n",
    "                pass\n",
    "            \n",
    "            # Fallback: count common delimiters\n",
    "            first_line = sample.split('\\n')[0] if '\\n' in sample else sample[:200]\n",
    "            counts = {\n",
    "                ',': first_line.count(','),\n",
    "                '\\t': first_line.count('\\t'),\n",
    "                '|': first_line.count('|'),\n",
    "                ';': first_line.count(';')\n",
    "            }\n",
    "            \n",
    "            # Return delimiter with highest count (if > 0)\n",
    "            max_delim = max(counts, key=counts.get)\n",
    "            if counts[max_delim] > 0:\n",
    "                return max_delim\n",
    "            \n",
    "            # Ultimate fallback\n",
    "            return ','\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: delimiter detection failed: {e}\")\n",
    "        return ','\n",
    "\n",
    "def load_file_v2(file_path: str, delimiter_hint: Optional[str], use_header: bool, \n",
    "                  id_column: Optional[str], id_index: Optional[int]) -> pd.DataFrame:\n",
    "    \"\"\"Version 2: Pre-validation approach with improved detection\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f'File not found: {file_path}')\n",
    "    \n",
    "    # Validate delimiter first\n",
    "    delimiter = validate_delimiter(delimiter_hint)\n",
    "    \n",
    "    # Auto-detect if needed\n",
    "    if delimiter is None:\n",
    "        delimiter = detect_delimiter_v2(file_path)\n",
    "        print(f\"Auto-detected delimiter for {os.path.basename(file_path)}: {repr(delimiter)}\")\n",
    "    else:\n",
    "        print(f\"Using validated delimiter for {os.path.basename(file_path)}: {repr(delimiter)}\")\n",
    "    \n",
    "    # Final safety check\n",
    "    if not delimiter or len(delimiter) == 0 or delimiter in ['\\n', '\\r']:\n",
    "        delimiter = ','\n",
    "        print(f\"Warning: Defaulting to comma delimiter\")\n",
    "    \n",
    "    header = 0 if use_header else None\n",
    "    df = pd.read_csv(file_path, sep=delimiter, header=header, dtype=str, engine='python')\n",
    "    \n",
    "    # Normalise whitespace and strip IDs\n",
    "    if use_header:\n",
    "        if id_column is None or id_column not in df.columns:\n",
    "            raise ValueError(f'ID column \"{id_column}\" not found. Available columns: {list(df.columns)}')\n",
    "        df[id_column] = df[id_column].astype(str).str.strip()\n",
    "        df_ids = df[[id_column]].copy()\n",
    "        df_ids.columns = ['ID']\n",
    "    else:\n",
    "        if id_index is None or id_index < 0 or id_index >= df.shape[1]:\n",
    "            raise ValueError(f'Invalid id_index {id_index}; file has {df.shape[1]} columns')\n",
    "        df_ids = df.iloc[:, [id_index]].copy()\n",
    "        df_ids.columns = ['ID']\n",
    "        df_ids['ID'] = df_ids['ID'].astype(str).str.strip()\n",
    "    \n",
    "    # Drop blank IDs\n",
    "    df_ids = df_ids[df_ids['ID'].astype(str).str.len() > 0]\n",
    "    \n",
    "    # Remove duplicates within each file\n",
    "    df_ids = df_ids.drop_duplicates().reset_index(drop=True)\n",
    "    return df_ids\n",
    "\n",
    "print('Pre-validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e56e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-Validation Test\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Pre-validation\")\n",
    "print(\"=\" * 60)\n",
    "try:\n",
    "    df1_v2 = load_file_v2(file1_path, delimiter1, use_header, id_column if use_header else None, id_index if not use_header else None)\n",
    "    df2_v2 = load_file_v2(file2_path, delimiter2, use_header, id_column if use_header else None, id_index if not use_header else None)\n",
    "    print(f\"✓ SUCCESS: File1={len(df1_v2)} IDs, File2={len(df2_v2)} IDs\")\n",
    "    theory2_success = True\n",
    "except Exception as e:\n",
    "    print(f\"✗ FAILED: {e}\")\n",
    "    theory2_success = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbb8907",
   "metadata": {},
   "source": [
    "## Loading Pathways\n",
    "Load each of the files and the pathways for the script to understand where to go for the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de28f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both files\n",
    "df1 = load_file(file1_path, delimiter1, use_header, id_column if use_header else None, id_index if not use_header else None)\n",
    "df2 = load_file(file2_path, delimiter2, use_header, id_column if use_header else None, id_index if not use_header else None)\n",
    "\n",
    "print(f'File1: {len(df1)} unique IDs')\n",
    "print(f'File2: {len(df2)} unique IDs')\n",
    "\n",
    "# Prepare sets for comparison (optionally normalised)\n",
    "if enable_normalization:\n",
    "    df1['ID_norm'] = df1['ID'].apply(normalize_id)\n",
    "    df2['ID_norm'] = df2['ID'].apply(normalize_id)\n",
    "    set1 = set(df1['ID_norm'])\n",
    "    set2 = set(df2['ID_norm'])\n",
    "else:\n",
    "    set1 = set(df1['ID'])\n",
    "    set2 = set(df2['ID'])\n",
    "\n",
    "matches_norm = sorted(list(set1 & set2))\n",
    "only_in_1_norm = sorted(list(set1 - set2))\n",
    "only_in_2_norm = sorted(list(set2 - set1))\n",
    "\n",
    "# Build display DataFrames preserving original ID values where possible\n",
    "if enable_normalization:\n",
    "    # Map normalised back to original examples for display\n",
    "    map1 = df1.drop_duplicates('ID_norm').set_index('ID_norm')['ID']\n",
    "    map2 = df2.drop_duplicates('ID_norm').set_index('ID_norm')['ID']\n",
    "    df_matches = pd.DataFrame({'ID': [map1.get(x, map2.get(x, x)) for x in matches_norm]})\n",
    "    df_only_in_1 = pd.DataFrame({'ID': [map1.get(x, x) for x in only_in_1_norm]})\n",
    "    df_only_in_2 = pd.DataFrame({'ID': [map2.get(x, x) for x in only_in_2_norm]})\n",
    "else:\n",
    "    df_matches = pd.DataFrame({'ID': sorted(list(set1 & set2))})\n",
    "    df_only_in_1 = pd.DataFrame({'ID': sorted(list(set1 - set2))})\n",
    "    df_only_in_2 = pd.DataFrame({'ID': sorted(list(set2 - set1))})\n",
    "\n",
    "print('Comparison complete.')\n",
    "df_summary = pd.DataFrame({\n",
    "    'Metric': ['Unique in File1', 'Unique in File2', 'Matches', 'Only in File1', 'Only in File2'],\n",
    "    'Count': [len(set1), len(set2), len(df_matches), len(df_only_in_1), len(df_only_in_2)]\n",
    "})\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77e2d33",
   "metadata": {},
   "source": [
    "## Table Results\n",
    "The below will create a table that shows what is similar or different from the two files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a378d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results with highlighting for non-matching IDs\n",
    "matches = df_matches['ID'].tolist()\n",
    "only_in_1_ids = df_only_in_1['ID'].tolist()\n",
    "only_in_2_ids = df_only_in_2['ID'].tolist()\n",
    "non_matching_ids = set(only_in_1_ids + only_in_2_ids)\n",
    "\n",
    "def highlight_non_matching(s):\n",
    "    return ['background-color: none' if value in non_matching_ids else '' for value in s]\n",
    "\n",
    "styled_all = pd.concat([\n",
    "    pd.DataFrame({'ID': matches, 'Status': ['MATCH'] * len(matches)}),\n",
    "    pd.DataFrame({'ID': only_in_1_ids, 'Status': ['ONLY_IN_FILE1'] * len(only_in_1_ids)}),\n",
    "    pd.DataFrame({'ID': only_in_2_ids, 'Status': ['ONLY_IN_FILE2'] * len(only_in_2_ids)})\n",
    "], ignore_index=True)\n",
    "\n",
    "styled_all = styled_all.style.apply(highlight_non_matching, subset=['ID']).apply(lambda s: ['color: #2b7' if v=='MATCH' else 'color: #cc8787' for v in s], subset=['Status'])\n",
    "styled_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8a9b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.bar(df_summary[\"Metric\"], df_summary[\"Count\"], color=\"#4c8bf5\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"ID Comparison Summary\")\n",
    "ax.set_ylim(0, max(df_summary[\"Count\"]) * 1.1)\n",
    "\n",
    "for i, v in enumerate(df_summary[\"Count\"]):\n",
    "    ax.text(i, v + 0.5, str(v), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d109ea",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "Report outputs from the comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d51479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save reports next to input files\n",
    "base_dir = os.path.dirname(file1_path) or os.getcwd()\n",
    "out_matches = os.path.join(base_dir, './Outputs/id_matches.csv')\n",
    "out_only_in_1 = os.path.join(base_dir, './Outputs/id_only_in_file1.csv')\n",
    "out_only_in_2 = os.path.join(base_dir, './Outputs/id_only_in_file2.csv')\n",
    "out_summary = os.path.join(base_dir, './Outputs/id_diff_summary.csv')\n",
    "\n",
    "df_matches.to_csv(out_matches, index=False)\n",
    "df_only_in_1.to_csv(out_only_in_1, index=False)\n",
    "df_only_in_2.to_csv(out_only_in_2, index=False)\n",
    "df_summary.to_csv(out_summary, index=False)\n",
    "\n",
    "print('Saved:')\n",
    "print(out_matches)\n",
    "print(out_only_in_1)\n",
    "print(out_only_in_2)\n",
    "print(out_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
